{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8018f197",
   "metadata": {},
   "source": [
    "- Andrea De Vita\n",
    "- Enrico Lupi\n",
    "- Manfredi Miranda\n",
    "- Francesco Zane\n",
    "\n",
    "-----------------------\n",
    "\n",
    "# Streaming Processing of the QUAX Experiment Data for the Detection of Galactic Axions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b81bd98",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The axion is a hypothetical particle introduced to solve the strong CP problem of Quantum Chromo Dynamics. It is speculated that axions may also constitute the dark matter content in our galaxy. The [QUAX](https://www.pd.infn.it/eng/quax/) (QUaerere AXions) experiment aims at detecting this particle by using a copper cavity immersed in a static magnetic field of 8.1 T, cooled down at a working temperature of about 150 mK.\n",
    "\n",
    "The goal of this project is to create a quasi real-time processing chain of the data produced by the QUAX experimental apparatus and a live monitoring system of the detector data, using [Apache Kafka](https://kafka.apache.org/) and [Apache Spark](https://spark.apache.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25ac8d9",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction) <br>\n",
    "    1.1. [Experiment](#intro_experiment) <br>\n",
    "    1.2. [Data Structure](#intro_data_structure) <br>\n",
    "    1.3. [Cluster](#intro_cluster) <br>\n",
    "2. [Data Processing](#processing) <br>\n",
    "    2.1. [Pipeline Overview](#pipeline) <br>\n",
    "    2.2. [Kafka - Streaming Data](#kafka) <br> \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.1. [Kafka Topics](#kafka_topic) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.2. [Data Pre-processing](#kafka_preprocessing) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.2.3. [Producer](#kafka_producer) <br>\n",
    "    2.3. [Spark - Distributed Processing](#spark) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.1. [Spark Structured Streaming](#spark_streaming) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.2. [FFT and Averaging](#spark_fft) <br>\n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2.3.3. [Output Message](#spark_output) <br>\n",
    "    2.4. [Live Plot and Monitoring](#live_plot) <br>\n",
    "3. [Performance Study](#performance) <br>\n",
    "    3.1. [Kafka Message Size](#test_kafka_msgsize) <br>\n",
    "    3.2. [Number of Partitions](#test_partitions)<br>\n",
    "    3.3. [Trigger *ProcessingTime* and *maxOffsetsPerTrigger*](#test_trigger)<br>\n",
    "4. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398118cf",
   "metadata": {},
   "source": [
    "## 1. Introduction <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa93b89",
   "metadata": {},
   "source": [
    "### 1.1. Experiment <a name=\"intro_experiment\"></a>\n",
    "\n",
    "\n",
    "![quax lab](Images\\labQuax.webp)\n",
    "\n",
    "The QUAX experiment at Legnaro INFN Laboratories aims at axion detection by using a copper cavity immersed in a static magnetic field of 8.1T, cooled down at a working temperature of about 150mK. The axion is expected to couple with the spin of the electron, interacting with the cavity and inducing a radio-frequency that can be sensed via a Josephson parametric amplifier. For a given configuration of the RF cavity, a scan of the phase of the electromagnetic field is performed to be able to possibly identify a localised excess, a hint of the coupling of an axion with the photon. \n",
    "\n",
    "The data acquisition system of the QUAX experiment generates two streams of digitized reading of the amplifiers, representing the real and imaginary components of the measured phase. To improve the signal over noise ratio, a QUAX data-taking run extends over a long time (up to weeks), repeating the scans over multiple times. Data are saved locally on the DAQ servers in the form of binary files, each corresponding to a multitude of continuous scans performed in the entire frequency range. A single pair of raw files is thus representative of only a few seconds of data taking, but are already including several (thousands) scans. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe23e5c",
   "metadata": {},
   "source": [
    "### 1.2. Data Structure <a name=\"intro_data_structure\"></a>\n",
    "\n",
    "The dataset is composed of 2 sets (named duck_i and duck_q respectively) of .dat binary files, each one comprised of a continuous series of ADC readings from the amplifier. Each ADC reading is written in the raw files as a 32 bit floating point value. The ADC readout frequency is 2 × 10<sup>6</sup> Hz (2 MegaSample per second, or 2MS/s), thus resulting in a raw data throughput of 128 Mbps (16 MB/s). During data taking the readouts are formatted in .dat file such that each file is comprised of 8193 × 2<sup>10</sup> samples. This results in producing a pair of .dat files (duck_i and duck_q) every 4.2 s.\n",
    "\n",
    "The dataset is provided on a cloud storage s3 bucket hosted on Cloud Veneto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf733f7",
   "metadata": {},
   "source": [
    "### 1.3. Cluster <a name=\"intro_cluster\"></a>\n",
    "\n",
    "This project has been done on a cluster composed by 4 virtual machines, each with 4 VCPUs with 25 GB disk space and 8 GB RAM each. The virtual machines are hosted on [CloudVeneto](https://cloudveneto.it/), an OpenStack-based cloud managed by University of Padova and INFN. Spark version 3.3.2 (using Scala version 2.12.15) and Kafka version 3.4.0 will be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5355d",
   "metadata": {},
   "source": [
    "## 2. Data Processing <a name=\"processing\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6d1aa",
   "metadata": {},
   "source": [
    "The processing of the raw data is comprised of two phases:\n",
    "1. Run a Fourier transform on each scan to move from the time domain to the frequency domain\n",
    "2. Average (in bins of frequency) all scans in a data-taking run, to extract a single frequency scan\n",
    " \n",
    "This procedure is highly parallelizable, and should be implemented in a quasi-online pipeline for two main reasons:\n",
    "1. Monitoring the scans during the data taking to promptly spot and identify possible issues in the detector setup or instabilities in the condition of the experiment\n",
    "2. Data is continuously produced with a very large rate, and the local storage provided by the DAQ server of the QUAX experiment is not really suited for large-volume and long-lasting datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93512045",
   "metadata": {},
   "source": [
    "### 2.1. Pipeline Overview <a name=\"pipeline\"></a>\n",
    "\n",
    "The data processing pipeline will be implemented as follows:\n",
    "- Each pair of files is unpacked according to their schema and split into scans.\n",
    "- Data is produced to a Kafka topic by a stream-emulator script every 5 seconds to simulate the fixed ADC scanning rate and the fixed size of files written to disk. \n",
    "- The processing of each file runs is performed in a distributed framework using pySpark: for each scan, a FFT is executed in parallel and the results of all FFTs are averaged.\n",
    "- The results are re-injected into a new Kafka topic hosted on the same brokers.\n",
    "- A final consumer performs the plotting, displaying live updates of the scans and continuously updating the entire \"run-wide\" scan using bokeh.\n",
    "\n",
    "The overall pipeline can be thus summarised as:\n",
    "![pipeline schema](Images\\Pipeline_Schema.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df89d6e",
   "metadata": {},
   "source": [
    "### 2.2. Kafka - Streaming Data <a name=\"kafka\"></a>\n",
    "\n",
    "Apache Kafka is an open-source distributed event streaming platform for high-performance data pipelines, streaming analytics, data integration, and mission-critical applications. As discussed previously, in this work it will be used to handle the live streaming of data from the DAQ servers all the way to the final live plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b255b",
   "metadata": {},
   "source": [
    "#### 2.2.1. Kafka Topics <a name=\"kafka_topic\"></a>\n",
    "\n",
    "The first step is to create a topic on the broker to hold the data from the DAQ. We create it with 12 separate partitions and no replication. The meaning of the name *chunk_data* will be made clear in the next section.\n",
    "\n",
    "We will also create a second topic for later, aptly named *results*, where to publish the results of the data processing, i.e. the FFT and averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648798d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the cluster to run admin functions\n",
    "kafka_admin = KafkaAdminClient(\n",
    "    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,\n",
    ")\n",
    "\n",
    "# define new topic to hold data\n",
    "topic_in = NewTopic(name='chunk_data',\n",
    "                    num_partitions=12, \n",
    "                    replication_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dba41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define new topic for the results\n",
    "topic_out = NewTopic(name='results',\n",
    "                     num_partitions=12, \n",
    "                     replication_factor=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aec9bd",
   "metadata": {},
   "source": [
    "#### 2.2.2. Data Preprocessing <a name=\"kafka_preprocessing\"></a>\n",
    "\n",
    "The size of the files produced by the DAQ is 32 MB, which means that each message handled by Kafka should be 64 MB as we need both the real and imaginary components. Unfortunately, the default message size in Kafka is only 1 MB. There are of course ways to circumvent this limit, namely:\n",
    " \n",
    "- at the broker level, changing the *replica.fetch.max.bytes* in the broker settings and increasing the *max.message.bytes* for the topic to the desired value\n",
    "- at the consumer level, increasing the *max.partition.fetch.bytes*, otherwise the consumer will fail to fetch these messages and will get stuck on processing\n",
    "- at the producer level, increasing the *max.request.size* to ensure large messages can be sent\n",
    "\n",
    "While this solution is possible, it is still against the philosophy of Kafka: sending large messages is considered inefficient as they should be huge in number but not in size.\n",
    "\n",
    "We thus decided to first unpack the data into slices and send a pair of real and imaginary slices as a message. Since for each FFT we want *n<sub>bins</sub>* = 3 × 2<sup>10</sup> = 3072 bins and we have a total of 8193 × 2<sup>10</sup> samples per file, the amount of slices to compute FFTs on for each file (and thus of mesages to be sent) is\n",
    "\n",
    "$$n_{slices} = \\cfrac{n_{samples}}{n_{bins}} = \\cfrac{8193 \\times 2^{10} }{3 \\times 2^{10}} = 2731$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa45ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all data from input files\\n\",\n",
    "real = bytearray(binary_data_real)\n",
    "imag = bytearray(binary_data_imm)\n",
    "\n",
    "# unpack data\n",
    "for f in range(n_slice):\n",
    "    r_bin = real[4*n_bins*f:4*n_bins*(f+1)] # one float every 4 bytes\n",
    "    i_bin = imag[4*n_bins*f:4*n_bins*(f+1)]\n",
    "\n",
    "# create kafka message\n",
    "msg = r_bin + i_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355bb09",
   "metadata": {},
   "source": [
    "#### 2.2.3. Producer <a name=\"kafka_producer\"></a>\n",
    "\n",
    "Lastly we can initialize the Kafka producer, the one responsible to read data from the files and actually sending it to the correct topic. The message, as described before, is given by two consecutives byte arrays containing the real and imaginary slices. The key, instead, contains the number of the file and of the particular slice contained in the message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba72111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kafka producer instance\n",
    "chunk_producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "\n",
    "# function to read files, unpack them and send them to Kafka\n",
    "def send_chunks(file_paths,dirPath,DAQ_period=5):\n",
    "    \n",
    "    # returns a list of lists each containing a pair of real and imaginary files \n",
    "    partners = sorted(find_partner(file_paths),\n",
    "                      key=lambda x: get_number_from_filename(x[0]))\n",
    "    \n",
    "    startTot = time.time()\n",
    "    wastedTime = 0\n",
    "    \n",
    "    for couple in partners: \n",
    "        start_time = time.time()\n",
    "            \n",
    "        # read all data from input files\n",
    "        couple=[dirPath+x for x in couple]\n",
    "        binary_data_real = read_binary_file(couple[0])\n",
    "        binary_data_imm = read_binary_file(couple[1])\n",
    "        \n",
    "        real = bytearray(binary_data_real)\n",
    "        imag = bytearray(binary_data_imm)\n",
    "        \n",
    "        file_num=int(couple[0][-9:-4])\n",
    "        \n",
    "        # unpack data\n",
    "        for f in range(n_slice):\n",
    "            r_bin = real[4*n_bins*f:4*n_bins*(f+1)] # one float every 4 bytes\n",
    "            i_bin = imag[4*n_bins*f:4*n_bins*(f+1)]\n",
    "\n",
    "            msg = r_bin + i_bin\n",
    "        \n",
    "            # key = file + bin number\n",
    "            key = (file_num).to_bytes(2, \"big\") + f.to_bytes(2, \"big\")\n",
    "           \n",
    "            print(Fore.RED +\"Sending file\",file_num,\"\\tslice number:\",f+1,end=\"\\r\")\n",
    "            \n",
    "            # send to Kafka topic\n",
    "            chunk_producer.send(topic = \"chunk_data\",\n",
    "                                key   = key,\n",
    "                                value = msg)\n",
    "        \n",
    "        end_time1 = time.time()\n",
    "        deltat = end_time1 - start_time\n",
    "        print(\"                                                                 \",end=\"\\r\")\n",
    "        print(\"File\", file_num,\"commissioned in\", round(deltat,3), \"s!\")\n",
    "        \n",
    "        chunk_producer.flush()  # Flush the producer after senting the entire file\n",
    "        \n",
    "        end_time2 = time.time()\n",
    "        deltat = end_time2 - start_time\n",
    "        print(\"                                                                 \")\n",
    "        print(\"File\", file_num,\"completed in \", deltat, \" sec!\")\n",
    "        print(\"------------------------------\")\n",
    "        \n",
    "        wastedTime+=(end_time2 - end_time1)\n",
    "        \n",
    "        # sleep to reproduce DAQ acquisition time\n",
    "        if deltat < DAQ_period:\n",
    "            time.sleep(DAQ_period - deltat)\n",
    "            \n",
    "    endTot = time.time()\n",
    "    deltaTot = endTot - startTot\n",
    "    \n",
    "    print(\"                                                                 \")\n",
    "    print(\"                                                                 \")\n",
    "    print(\"------------------------------\")\n",
    "    print(Fore.GREEN+\"Total time\", round(deltaTot,3), \"s!\")\n",
    "    print(Fore.RED +\"Wasted time\", round(wastedTime,3), \"s!\")\n",
    "    print(Fore.BLACK +\"------------------------------\")\n",
    "        \n",
    "send_chunks(file_paths,folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e520df",
   "metadata": {},
   "source": [
    "### 2.3. Spark - Distributed Processing <a name=\"spark\"></a>\n",
    "\n",
    "Apache Spark is an open-source unified analytics engine for large-scale data processing. As outlined in the overview, Spark will do the \"heavy lifting\" of the data processing by computing the FFTs for each slice in parallel and averaging them.\n",
    "\n",
    "The first step is to create a Spark application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4a4ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://10.67.22.8:7077\") \\\n",
    "        .appName(\"Spark structured streaming application\") \\\n",
    "        .config(\"spark.executor.memory\", \"1000m\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "        .config(\"spark.sql.execution.arrow.pyspark.fallback.enabled\", \"false\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", 12) \\\n",
    "        .config(\"spark.jars.packages\",\"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df53d89e",
   "metadata": {},
   "source": [
    "#### 2.3.1 Spark Structured Streaming <a name=\"spark_streaming\"></a>\n",
    "\n",
    "In order to deal with the quasi-continuous stream of data from Kafka we will use Spark Structured Streaming. This lets us use the DataFrame API and consider the incoming data as new rows of an unbound table.\n",
    "\n",
    "Given the simplicity and lack of multiple features of the data we could also have chosen to use Spark Streaming, which works by dividing the input data into a sequence micro-batches (DStream) that can be treated as static datasets. Unfortunately, Spark Streaming is considered deprecated, and as such the packages necessary to connect it to Kafka are not present in Spark version 3, which we are currently using.\n",
    "\n",
    "We first create an input (streaming) DataFrame subscribed to the *chunk_data* topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6906b8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDF = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS) \\\n",
    "    .option(\"kafkaConsumer.pollTimeoutMs\", 30_000) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .option(\"maxOffsetsPerTrigger\", 2000) \\\n",
    "    .option(\"subscribe\", \"chunk_data\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da0459f",
   "metadata": {},
   "source": [
    "Let's explain what some of these options are:\n",
    "- *startingOffset* refers to the start point when a query is started: either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition.\n",
    "- *kafkaConsumer.pollTimeoutMs* is the timeout in milliseconds to poll data from Kafka in executors.\n",
    "- *maxOffsetsPerTrigger* is the rate limit on maximum number of offsets processed per trigger interval. The specified total number of offsets will be proportionally split across topicPartitions of different volume.\n",
    "\n",
    "Note that *maxOffsetsPerTrigger* is equal to 2000, which means that we are never going to deal with batches bigger than a single file. This means, however, that it is possible to have batches containing slices of different files. This last case is not a problem, though, as the division in files depends only on the DAQ system and does not reflect any underlying physics.\n",
    "\n",
    "The input DataFrame from Kafka has the following schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccfcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87586501",
   "metadata": {},
   "source": [
    "#### 2.3.2 FFT and Averaging <a name=\"spark_fft\"></a>\n",
    "\n",
    "We are only interested in the *key* and *value* pair. We then unpack the *value* column into a list of floats and compute the Fourier Transform. Note that we have to explicitly declare the Fourier Transform as an UDF (User Defined Function) so that it can act on SQL columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50790771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a byte array into a list of float values\n",
    "def bytes_to_float32_list(bytes_value):\n",
    "    float_list = []\n",
    "    for i in range(0, len(bytes_value), 4): \n",
    "        float_value = sstruct.unpack('f', bytes_value[i:i+4])[0]\n",
    "        float_list.append(float_value)\n",
    "    \n",
    "    return float_list\n",
    "\n",
    "# Function to compute the Fourier transform of a given array\n",
    "def Fourier(x):\n",
    "    x = np.array(x)\n",
    "    # take real and imaginary components and get complex number  \n",
    "    z = to_complex(x)\n",
    "    \n",
    "    power = np.abs(np.fft.fft(z))**2\n",
    "    FS = fft_bandwidth\n",
    "    norm = n_bins * FS * np.sqrt(2)\n",
    "    normalized_power = power / norm\n",
    "    power_shifted = np.fft.fftshift(normalized_power)\n",
    "    \n",
    "    power_shifted = power_shifted.tolist()\n",
    "    \n",
    "    return(power_shifted)\n",
    "\n",
    "# Function to index elements in a list with file numbers\n",
    "def indexing(x,file_num):\n",
    "    k = []\n",
    "    for i in range(len(x)):\n",
    "        add = (f'{file_num}_{i}',x[i])\n",
    "        k.append(add)\n",
    "    return k\n",
    "\n",
    "# Function to extract the file number from a byte array (big-endian short)\n",
    "def extract_file_num(key_bytes):\n",
    "    return sstruct.unpack('>H', key_bytes[:2])[0]  # Unpack from big-endian short\n",
    "\n",
    "\n",
    "\n",
    "# schema for indexing UDF\n",
    "schema = StructType(\n",
    "        [\n",
    "                StructField(\"index\", StringType()),\n",
    "                StructField(\"x\", FloatType())\n",
    "        ]\n",
    ")\n",
    "\n",
    "# Define UDF\n",
    "fft_udf = udf(Fourier, ArrayType(FloatType()))\n",
    "extract_file_num_udf = udf(extract_file_num, IntegerType())\n",
    "indexing_udf = udf(indexing, ArrayType(schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ff8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UDFs to transform 'value' column in a list of Fourier transformed value\n",
    "streaming_df = inputDF.select('key', 'value')\n",
    "streaming_df = streaming_df.withColumn('float', bytes_to_float32_udf(streaming_df['value']))\n",
    "streaming_df = streaming_df.withColumn('fft', fft_udf(streaming_df['float']))\n",
    "\n",
    "# Extract file numbers from 'key' column\n",
    "streaming_df = streaming_df.withColumn('file_num', extract_file_num_udf(col('key')))\n",
    "\n",
    "# Apply UDF to index 'fft' column by 'file_num'\n",
    "streaming_df = streaming_df.withColumn('indexed_fft', indexing_udf(streaming_df['fft'],streaming_df['file_num']) )\n",
    "\n",
    "# Explode the 'indexed_fft' array to separate rows\n",
    "exploded_df = streaming_df.select('key', explode('indexed_fft').alias('indexed_fft'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd3f9c6",
   "metadata": {},
   "source": [
    "We then extract the file number information from the *key* and, after some transformations, obtain a row for each bin with the following schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0bd052",
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- indexed_fft: struct (nullable = true)\n",
    " |    |-- index: integer (nullable = true)\n",
    " |    |-- x: float (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a45cbd",
   "metadata": {},
   "source": [
    "where *indexed_fft* is a structure containing two elements:\n",
    "- *index*, the combination of the file number and bin number\n",
    "- *x*, the value fo the FFT in the specific bin for the specific file\n",
    "\n",
    "Finally, we compute the mean and standard deviation of the FFTs for each bin after grouping by the FFT index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eb0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'indexed_fft.indice' and calculate statistics\n",
    "result_df = exploded_df.groupBy(\"indexed_fft.index\").agg(\n",
    "    mean(\"indexed_fft.x\").alias(\"mean_x\"),\n",
    "    stddev(\"indexed_fft.x\").alias(\"stddev_x\"),\n",
    "    count(\"indexed_fft.x\").alias(\"count_x\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec02af33",
   "metadata": {},
   "source": [
    "#### 2.3.3 Output Message <a name=\"spark_output\"></a>\n",
    "\n",
    "The only step left is to produce a message to send Kafka in the *results* topic. This message is only sent if the whole file has been analyzed, i.e. if all 2731 slices have been used to compute the mean. \n",
    "\n",
    "The message will follow this schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b5d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "root\n",
    " |-- data: struct (nullable = false)\n",
    " |    |-- index: integer (nullable = true)\n",
    " |    |-- mean_x: double (nullable = true)\n",
    " |    |-- stddev_x: double (nullable = true)\n",
    " |    |-- count_x: integer (nullable = true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04f63ba",
   "metadata": {},
   "source": [
    "and be sent as Json file. In the options for the *writeStream* we have:\n",
    "- *trigger*, which controls how often to run a microbatch query periodically based on the *ProcessingTime*\n",
    "- *outputMode*, that defines what gets written out to the external storage. In particular, *update* means that only the rows that were updated in the Result Table since the last trigger will be written to the external storage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e23fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and structure the data for output as a single JSON message \n",
    "# when the mean is calculated from a full couple of file\n",
    "result_json_df = result_df.where(col('count_x')==2731) \\\n",
    "    .select(struct(\"index\", \"mean_x\", \"stddev_x\",\"count_x\").alias(\"data\"))\n",
    "\n",
    "# function to send data to Kafka as JSON messages\n",
    "def send_to_kafka(batch_df, batch_id):\n",
    "    batch_json = batch_df.toJSON().collect()\n",
    "    all_data_json = json.dumps([json.loads(row) for row in batch_json])\n",
    "    \n",
    "    producer = KafkaProducer(bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS)\n",
    "    producer.send(\"results\", value=all_data_json.encode(\"utf-8\"))\n",
    "    producer.close()\n",
    "\n",
    "# Write the JSON data to Kafka as a single message\n",
    "query = result_json_df.writeStream \\\n",
    "    .trigger(processingTime=\"12 seconds\")\\\n",
    "    .outputMode(\"update\") \\\n",
    "    .foreachBatch(send_to_kafka) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9eefb",
   "metadata": {},
   "source": [
    "### 2.4 Live Plot and Monitoring <a name=\"live_plot\"></a>\n",
    "\n",
    "The only step left is to plot the results and monitor them in real time. To handle live plotting we will use Bokeh. \n",
    "\n",
    "We will show both the results for the latest batch sent to Kafka and the cumulative average of all the files processed up until now with their relative errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f15e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_kafka(): # Here we define a consumer to read the Json sent from the Spark notebook \n",
    "    ...           # so we can turn it into a dictionary\n",
    "    \n",
    "# Then we define four Bokeh structure that can be updated\n",
    "stream_source = ColumnDataSource(data = {'freq': [], 'fft' : []})\n",
    "\n",
    "# Define a callback function to update the plot data\n",
    "def update():\n",
    "    ...\n",
    "    kafka_data = read_kafka()\n",
    "    stream_source.data['fft'] = kafka_data['mean']\n",
    "    ...\n",
    "    \n",
    "\n",
    "# Set up a periodic callback to update the plot every 5 seconds\n",
    "callback = PeriodicCallback(update, 5_000) \n",
    "\n",
    "\n",
    "callback.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59bc0c",
   "metadata": {},
   "source": [
    "The final result looks like this:\n",
    "\n",
    "<video width=\"826\" height=\"504\" \n",
    "       src=\"./Images/live_plot.webm\"  \n",
    "       controls>\n",
    "</video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5502a3c9",
   "metadata": {},
   "source": [
    "Lastly, we develop a method to save the results as a csv file for storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27693d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pandas storage for the final results\n",
    "storage = pd.DataFrame({'freq':xaxis,'fft':np.zeros(n_bins), 'std':np.zeros(n_bins)})\n",
    "\n",
    "def update():\n",
    "    ...\n",
    "    storage['fft'] = cum_y\n",
    "    storage['std'] = cum_sigma\n",
    " \n",
    "\n",
    "callback.stop()\n",
    "storage.to_csv('final_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bb41b0",
   "metadata": {},
   "source": [
    "## 3. Performance Study <a name=\"test\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221b8e6e",
   "metadata": {},
   "source": [
    "We will now look into how well our pipeline performs and how tuning the different parameters affects its execution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e52f789",
   "metadata": {},
   "source": [
    "### 3.1. Kafka Message Size <a name=\"test_kafka_msgsize\"></a>\n",
    "\n",
    "The first parameter we want to test is the size of the Kafka messages. In order to do so, we change the code reported above to make it more flexible so that we can change the number of slices per message sent from the DAQ-emulator; this number varies from a minimum of 1 to a maximum of 40, so that the final message is still below 1 MB size.\n",
    "\n",
    "The results in this section have been obtained by running the code for all 16 raw data files and averaging the execution times for each one of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3820b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "slices_per_msg = 1\n",
    "msg_number = math.ceil(n_slice/slices_per_msg)\n",
    "\n",
    "# read data from input files\n",
    "real = bytearray(binary_data_real)\n",
    "imag = bytearray(binary_data_imm)\n",
    "\n",
    "# unpack data\n",
    "for f in range(msg_number):\n",
    "    start = 4*n_bins*slices_per_msg*f\n",
    "    end = 4*n_bins*slices_per_msg*(f+1)\n",
    "    if end > 4*n_samples:\n",
    "        end = 4*n_samples\n",
    "        \n",
    "    # create Kafka message\n",
    "    r_bin = real[start:end]\n",
    "    i_bin = imag[start:end]\n",
    "    msg = r_bin + i_bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e56cab",
   "metadata": {},
   "source": [
    "As a preliminary test, we first run the code without actually sending any message to Kafka, in order to gauge how much time is needed to read the files and divide them in slices. *A priori* it could be argued that for very large files this procedure could take a very long time, and thus it should be parallelized as well. In fact, after testing we find that in our concrete case the average execution time is only $\\bar{t}_{no \\, msg \\, sent} = 0.27 \\pm 0.02 $ s. Such a low time means that even reading the data on a single machine is completely fine.\n",
    "\n",
    "We then test the actual time needed to send messages, using 1, 10, 20, 30 and 40 slices per message, which correspond to sizes of 24, 240, 480, 720 and 960 KB respectively. We obtain the following results:\n",
    "- $\\bar{t}_{\\,\\,1 \\, slice/msg} = 16.5 \\pm 0.2 $ s\n",
    "- $\\bar{t}_{   10 \\, slice/msg} = 15.5 \\pm 0.1 $ s\n",
    "- $\\bar{t}_{   20 \\, slice/msg} = 15.1 \\pm 0.1 $ s\n",
    "- $\\bar{t}_{   30 \\, slice/msg} = 15.0 \\pm 0.1 $ s\n",
    "- $\\bar{t}_{   40 \\, slice/msg} = 15.6 \\pm 0.1 $ s\n",
    "\n",
    "![kafka slices per message](Images\\KafkaTest_SlicePerMsg.png)\n",
    "\n",
    "The data seems to follow a parabolic trend with a minimum around 30 slices per message: both a low number of large messages an a high number of small messages are not ideal. The difference, though, is not very significant (only of 9%), and external factors like the quality of the connection could play a bigger role in the performance.\n",
    "\n",
    "The most important thing to note, however, is the absolute value of these times: even in our best-case scenario, the execution time is still around 15 second, more than three times the time needed to produce new files (4.2 s). This means that the data transmission will be a huge bottleneck in the pipeline and will slow down the rest of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d188a91b",
   "metadata": {},
   "source": [
    "## 3.2. Number of Partitions <a name=\"test_partitions\"></a>\n",
    "\n",
    "The second parameter we tested is the number of partitions of the Kafka topic *chunk_data*, which also corresponds to the number of Spark partititons. We first checked with 4 and later with 12 partitions: *a priori* we expect this second value to be optimal, as we would have a one-to-one correspondence between the partitions and the available cores (four cores for each of the three VCPUs). It is important to note that in order to increase the number of partitions we need also to increase the *spark.executor.memory* to 1 GB, otherwise it would lead to a crash.\n",
    "\n",
    "Unfortunately, we do not see any real improvement: this is due to the fact that, as discussed above, Kafka communication is the real limiting factor of the pipeline. Even with a non-ideal number of partitions Spark is still faster than Kafka and so the performance stays roughly the same, around 200 records per second for both the input and processing rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18865b04",
   "metadata": {},
   "source": [
    "## 3.3. Trigger *ProcessingTime* and *maxOffsetsPerTrigger* <a name=\"test_trigger\"></a>\n",
    "\n",
    "After we studied how the performance is affected by changing the *maxOffsetsPerTrigger* value when creating the input DataFrame and the Trigger *ProcessingTime* when computing the results. We will analyze the following performance metrics:\n",
    "- Input rate: it specifies how much data is flowing into Structured Streaming from Kafka\n",
    "- Processing rate: it is how quickly we were able to analyze that data\n",
    "- Input Rows: it defines the total number of records processed in a trigger. The more the trigger time, the more will be the Input Rows\n",
    "- Batch Duration: It defines the process duration of each batch\n",
    "- Operation Duration: It defines the amount of time taken to perform the below operations (in milliseconds).\n",
    "    - addBatch: Time taken to read the micro-batch’s input data from the sources, process it, and write the batch’s output to the sink. This should take the bulk of the micro-batch’s time.\n",
    "    - getBatch: Time taken to prepare the logical query to read the input of the current micro-batch from the sources.\n",
    "    - latestOffset & getOffset: Time taken to query the maximum available offset for this source.\n",
    "    - queryPlanning: Time taken to generates the execution plan.\n",
    "    - walCommit: Time taken to write the offsets to the metadata log.\n",
    "\n",
    "We start using a *maxOffsets* of 1000 and triggering every 20 seconds.\n",
    "![offset-1000_trigger-20](Images\\offset-1000_trigger-20.png)\n",
    "\n",
    "We can observe that both Input Rate and Input Rows reach a constant plateau around 50 records/sec and 1000 records respectively. This is because the triggering time is too long for this *maxOffsets* so that we always reach the maximum amount of records allowed inside one batch. This is of course not ideal as it means we have dead time between one batch and the next, as one batch is closed and finishes processing but we still need to wait for the net trigger. We can confirm this by visualizing the batch processing timeline, where we can see that each job is time separated from the others:\n",
    "![deadtime](Images\\deadtime.jpg)\n",
    "\n",
    "Then we evaluate a *maxOffsets* of 2000 and triggering every 5 seconds, the opposite situation as the one above:\n",
    "![offset-2000_trigger-5](Images\\offset-2000_trigger-5.png)\n",
    "\n",
    "In this case we can see that Input Rate and Input Rows are higher but oscillate significantly. The Input  Rate, in particular, is once again limited by Kafka: we are very close to the expected limit of 165 records/sec (2731 messages in 16.5 seconds on average). For the Input Rows, instead, we see that at first we have a higher value (still lower than the set maximum), but then Spark automatically reduces. This is because the processing time for each batch (as we can see in Batch Duration) exceeds the 5 seconds limit of the trigger, so Spark tries to reduce the size of each batch to fit into this time window.\n",
    "\n",
    "The objective now is thus to find the correct equilibrium in the parameters so that the batches have the correct size and the trigger is neither too long nor too short. After some trials, the optimum conditions have been reached with a *maxOffsets* of 2000 and triggering every 12 seconds:\n",
    "![best_offset-2000_trigger-12](Images\\best_offset-2000_trigger-12.jpg)\n",
    "\n",
    "We can see that once again Input Rate and Input Rows are roughly constant and very close to the maximum values. The processing Rate is also higher than the previous cases, reaching almost 250 records/sec.\n",
    "\n",
    "Using this last configuration, all 16 files are processed in 4 mins and 30 sec. This time is mostly due to the low input rate caused by Kafka. Even setting that aside, though, the time to analyze one batch is roughly 8 sec, so the time to analyze a whole file will surely be higher than the time necessary to produce a new one, so the total process would lag behind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b7ce1",
   "metadata": {},
   "source": [
    "## 4. Conclusion <a name=\"conclusion\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb5c933",
   "metadata": {},
   "source": [
    "In this work we have developed a data processing pipeline to handle and analyze data produced by the QUAX experiment at Legnaro INFN Laboratories. In this pipeline the raw data is produced to a Kafka topic and then read by a Spark streming application, which computes the FFT and averages the results. Finally, these results are outputted to another topic and used to perform a live plot for monitoring. <br>\n",
    "The performances of the pipeline have later been studied and optimized as a function of some parameters.\n",
    "\n",
    "The final configuration of the pipeline, which uses 12 Kafka and Spark partitions, a *maxOffsetsPerTrigger* of 2000 a Trigger *ProcessingTime* is able to analyze all 16 files available for testing in 4 minutes and 30 seconds.\n",
    "\n",
    "This result is far from optimal, as the files are produced in only a quarter of the processing time (~ 67 seconds). The first \"culprit\" is Kafka, which is not able to send messages at a fast enough rate to keep up with the production rate, but Spark is not working at the desired rate, too. <br>\n",
    "A first approach to solve this problem would thus be to use a different method to link the source to the Spark Stream instead of Kafka. Moreover, Spark could be further optimized and a more powerful cluster could be used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
